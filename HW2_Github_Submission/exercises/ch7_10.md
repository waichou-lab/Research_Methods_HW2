# Exercise 7.10 — Latent Semantic Analysis (LSA)

**(a)** For occurrence matrix X ∈ R^{m×n} (m words, n documents):
- XX^T ∈ R^{m×m}: word–word co-occurrence (counts across documents) — measures similarity between words. 
- X^T X ∈ R^{n×n}: document–document Gram matrix — similarity between documents (inner products of document vectors). fileciteturn6file11

**(b)** Use SVD: X = U Σ V^T. The best k-dimensional approximation (Eckart–Young) is X_k = U_k Σ_k V_k^T. The columns of U_k (the top k left singular vectors) give k vectors in R^m that best approximate the document space in Frobenius norm. See Theorem 7.1. fileciteturn6file11

**(c)** Canonical correlation (cross-language): maximize (X1^T v1)·(X2^T v2) / (||v1|| ||v2||). This can be recast and solved via SVD on suitably whitened cross-covariance (or by taking SVD of X1^T X2 after appropriate normalization). The top singular vectors give correlated directions between languages. fileciteturn6file11
